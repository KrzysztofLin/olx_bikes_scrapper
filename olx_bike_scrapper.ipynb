{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "olx_bike_scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrzysztofLin/olx_bikes_scrapper/blob/main/olx_bike_scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Settings\n",
        "\n"
      ],
      "metadata": {
        "id": "SH7JQbF4tJP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WhLrqfybMPcD"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from requests import get\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "import itertools\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OLX_MAIN_ADDRESS = \"https://www.olx.pl\"\n",
        "MAIN_CATEGORY = \"/d/sport-hobby/rowery\"\n",
        "ROOT_URL = OLX_MAIN_ADDRESS + MAIN_CATEGORY\n",
        "OFFERT = \"/d/oferta\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/102.0.5005.72 Safari/537.36 \",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,\"\n",
        "              \"application/signed-exchange;v=b3;q=0.9 \"\n",
        "}\n",
        "\n",
        "LIST_OF_MANUFACTURES = [\n",
        "'unibike',\n",
        "'kross',\n",
        "'trek',\n",
        "'scott',\n",
        "'haibike',\n",
        "'specialized',\n",
        "'cann?ondale',\n",
        "'ktm',\n",
        "'romet',\n",
        "'kellys',\n",
        "'kands',\n",
        "'lazzaro',\n",
        "'merida',\n",
        "'gt',\n",
        "'dartmoor',\n",
        "'giant',\n",
        "'kona',\n",
        "'author',\n",
        "'cube',\n",
        "'canyon',\n",
        "'accent',\n",
        "'oreba',\n",
        "'boa?rdman',\n",
        "'ghost',\n",
        "'focus',\n",
        "'stevens',\n",
        "'marin',\n",
        "\"b[i']?twin\",\n",
        "'ridley',\n",
        "'triban']\n",
        "\n",
        "COMPILED_LIST_OF_MANUFACTURES = re.compile(\"|\".join([\"\\\\b\"+i+\"\\\\b\" for i in LIST_OF_MANUFACTURES]))\n",
        "LIST_OF_WHEEL_SIZES = r\"(24|26|27.5|27,5|27|28|29)\"\n",
        "LIST_OF_PRODUCTION_YEAR = r\"20[12][0-9]\"\n",
        "BIKE_WEIGHT_REGEX = r\"([0-9]?[0-9][.,]?[0-9]?[0-9][ ]?kg|waga[:]?[ ]?[0-9]?[0-9][.,]?[0-9]?)\"\n"
      ],
      "metadata": {
        "id": "h5Z6zcTaM7Tc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to find categories"
      ],
      "metadata": {
        "id": "RCzieXTMxL8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_bikes_categories():\n",
        "  bike_categories = dict()\n",
        "  html = get(url=ROOT_URL, headers=HEADERS).text\n",
        "  soup = Soup(html, 'html.parser')\n",
        "  category = MAIN_CATEGORY+\"/rowery\"\n",
        "  category_number = 1\n",
        "\n",
        "  for links in soup.find_all('a'):\n",
        "    address = links.get('href')\n",
        "    if address:\n",
        "      if category in address:\n",
        "        address_name = address.replace('/d/sport-hobby/rowery/','').replace('/','').replace('-',' ')\n",
        "        bike_categories[category_number] = address_name, OLX_MAIN_ADDRESS + address\n",
        "        category_number += 1\n",
        "  bike_categories[category_number] = 'rowery gravelowe', 'https://www.olx.pl/d/sport-hobby/rowery/q-gravel/'\n",
        "\n",
        "  return bike_categories"
      ],
      "metadata": {
        "id": "NER6Sm0LO991"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_all_ulrs_on_page(page, subcategory_html):\n",
        "  url=subcategory_html+ f\"/?page={page}\"\n",
        "  url_list = []\n",
        "  html = get(url=url, headers=HEADERS).text\n",
        "  soup = Soup(html, 'html.parser')\n",
        "  for links in soup.find_all('a'):\n",
        "    address = links.get('href')\n",
        "    if address:\n",
        "      if OFFERT in address:\n",
        "        url_list.append(OLX_MAIN_ADDRESS+address)\n",
        "  return url_list\n",
        "\n",
        "def find_all_offerts_in_category(subcategory_html, page_range = 10):\n",
        "  pool = mp.Pool(mp.cpu_count())\n",
        "  results = pool.starmap(_find_all_ulrs_on_page, [(page, subcategory_html) for page in range(page_range)])\n",
        "  pool.close()\n",
        "  print(\"------\")\n",
        "  fresults = list(itertools.chain(*results))\n",
        "  return fresults\n",
        "\n",
        "def _find_info_in_url(current_url):\n",
        "  html = get(url=current_url, headers=HEADERS).text\n",
        "  soup = Soup(html, 'html.parser')\n",
        "  selector = 'div.css-g5mtbi-Text'\n",
        "  found = soup.select(selector)\n",
        "  description = [x.text.split(';')[-1].strip().replace('\\n', ' ') for x in found]\n",
        "  if soup:\n",
        "    try:\n",
        "      title = soup.find(\"h1\").text\n",
        "      price = float(soup.find(\"h3\").text[:-2].replace(\" \", '').replace(',','.'))\n",
        "      return current_url, title,  price,  description[0]\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    except ValueError:\n",
        "      pass\n",
        "\n",
        "def search_offerts(url_list):\n",
        "  pool = mp.Pool(mp.cpu_count())\n",
        "  results = pool.map(_find_info_in_url, [current_url for current_url in url_list])\n",
        "  pool.close()\n",
        "  return results\n",
        "\n",
        "def _find_photos_in_url(current_url: str):\n",
        "  html = get(url=current_url, headers=HEADERS).text\n",
        "  soup = Soup(html, 'html.parser')\n",
        "  bike_photo_addresses = [soup.find('img', attrs= {'src': re.compile(\"https://ireland\")}).get('src')]\n",
        "  for link in soup.find_all('img', attrs= {'data-src': re.compile(\"https://ireland\")}):\n",
        "    bike_photo_addresses.append(link.get('data-src'))\n",
        "  return current_url, bike_photo_addresses\n",
        "\n",
        "def search_offerts2(url_list):\n",
        "  pool = mp.Pool(mp.cpu_count())\n",
        "  results = pool.map(_find_photos_in_url, [current_url for current_url in url_list])\n",
        "  pool.close()\n",
        "  return results\n",
        "\n",
        "'''\n",
        "for localization search\n",
        "    #selector = 'p.css-xl6fe0-Text.eu5v0x0'\n",
        "    #selector = 'div.css-1q7h1ph'\n",
        "    #found = soup.select(selector)\n",
        "    #print(found)\n",
        "    person = {}\n",
        "    for div in soup.find_all(class_ = 'css-1dp6pbg'):#.find(class_= 'css-xl6fe0-Text eu5v0x0'):\n",
        "      print(div)\n",
        "      for link in div.find(class_= 'css-xl6fe0-Text eu5v0x0'):\n",
        "        print(link) \n",
        "'''"
      ],
      "metadata": {
        "id": "BvXIWj3mE_zU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b3d1ef47-4e35-41f3-8426-8b75242b29f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor localization search\\n    #selector = 'p.css-xl6fe0-Text.eu5v0x0'\\n    #selector = 'div.css-1q7h1ph'\\n    #found = soup.select(selector)\\n    #print(found)\\n    person = {}\\n    for div in soup.find_all(class_ = 'css-1dp6pbg'):#.find(class_= 'css-xl6fe0-Text eu5v0x0'):\\n      print(div)\\n      for link in div.find(class_= 'css-xl6fe0-Text eu5v0x0'):\\n        print(link) \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bike_categories = find_bikes_categories()\n",
        "url_list = find_all_offerts_in_category(subcategory_html = bike_categories[5][1], page_range = 2 )\n",
        "url_list = ['https://www.olx.pl/d/oferta/trek-marlin-6-29-hydraulika-CID767-IDQaktC.html']\n",
        "bikes_photo_list = search_offerts2(url_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OP982IuInex",
        "outputId": "a376b146-1e61-4fab-f328-c26bfea46a29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import shutil\n",
        "\n",
        "for bike_url_and_photos in bikes_photo_list:\n",
        "  file_name = str(bike_url_and_photos[0]).replace(OLX_MAIN_ADDRESS+OFFERT+'/', '').replace('.html', '')\n",
        "  for index, photo_url in enumerate(bike_url_and_photos[1:][0]):\n",
        "    photo_filename = file_name + '_' +str(index) + '.jpg'\n",
        "    res = requests.get(photo_url, stream = True)\n",
        "    \n",
        "    if res.status_code == 200:\n",
        "      with open(photo_filename, 'wb') as f:\n",
        "          shutil.copyfileobj(res.raw, f)"
      ],
      "metadata": {
        "id": "KefJFvYCMsr-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "regex to find size"
      ],
      "metadata": {
        "id": "JhX_KsljObIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_frame_size(text_to_analysis: List[str]) -> list:\n",
        "  frame_size = list()\n",
        "  r1 = None\n",
        "  for text in text_to_analysis:\n",
        "    try:\n",
        "      r1 = re.findall(r\"ramy[ ]?.?[ ]?(17|18|19|20|21|22|23|48|49|50|51|52|53|54|55|56|57|58|59|60|62|64|xs|s|m|xl|l)\", text)\n",
        "      if r1 == []:\n",
        "        r1 = re.findall(r\"rama[ ]?.?[ ]?(17|18|19|20|21|22|23|48|49|50|51|52|53|54|55|56|57|58|59|60|62|64|xs|s|m|xl|l)\", text)\n",
        "        if r1 == []:\n",
        "          r1 = re.findall(r\"rozmiarz?e?[ ]?.?[ ]?(17|18|19|20|21|22|23|48|49|50|51|52|53|54|55|56|57|58|59|60|62|64|xs|s|m|xl|l)\", text)\n",
        "          if r1 == []:\n",
        "            r1 = re.findall(r\"roz[ ]?.?[ ]?(17|18|19|20|21|22|23|48|49|50|51|52|53|54|55|56|57|58|59|60|62|64|xs|s|m|xl|l)\", text)\n",
        "    except ValueError:\n",
        "      pass\n",
        "    except TypeError:\n",
        "      pass\n",
        "    sizes = []\n",
        "    if r1:\n",
        "      my_result = max(set(r1), key=r1.count) \n",
        "      try:  \n",
        "        sizes = re.findall(\"\\d+\", my_result)[0]\n",
        "      except IndexError:\n",
        "        sizes = my_result[-2:]\n",
        "    if sizes:\n",
        "      frame_size.append(sizes)\n",
        "    else:\n",
        "      frame_size.append('')\n",
        "  return frame_size"
      ],
      "metadata": {
        "id": "4x0-slVzbjN4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_bike_weight(text_to_analysis):\n",
        "  bikes_weight = _find_attribiute(text_to_analysis, regex = BIKE_WEIGHT_REGEX )\n",
        "  return bikes_weight\n",
        "\n",
        "def find_bike_production_year(text_to_analysis):\n",
        "  return _find_attribiute(text_to_analysis, regex = LIST_OF_PRODUCTION_YEAR)\n",
        "\n",
        "def find_wheel_size(text_to_analysis):\n",
        "  return _find_attribiute(text_to_analysis, regex = LIST_OF_WHEEL_SIZES)\n",
        "\n",
        "def find_producent(text_to_analysis):\n",
        "  return _find_attribiute(text_to_analysis, regex = COMPILED_LIST_OF_MANUFACTURES)\n",
        "\n",
        "def _find_attribiute(text_to_analysis, regex):\n",
        "  bikes_atrribiute = list()\n",
        "  for text in text_to_analysis:\n",
        "    attribiute = None\n",
        "    try:\n",
        "      atrribiute = re.findall(regex, text)\n",
        "      if atrribiute:\n",
        "        bikes_atrribiute.append(atrribiute[0])\n",
        "      else:\n",
        "        bikes_atrribiute.append('')\n",
        "    except ValueError:\n",
        "      bikes_atrribiute.append('')\n",
        "    except AttributeError:\n",
        "      bikes_atrribiute.append('')\n",
        "    except TypeError:\n",
        "      bikes_atrribiute.append('')\n",
        "  return bikes_atrribiute"
      ],
      "metadata": {
        "id": "H0yGec1AF5FY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_category_and_page_range_choice(bike_categories):\n",
        "  for key, values in bike_categories.items():\n",
        "    print(f\"{key}: {values[0]}\")\n",
        "\n",
        "  user_category_choice = int(input(f'Wybierz kategorię od 1 do {list(bike_categories)[-1]}: '))\n",
        "  user_page_range_choice = int(input(f'Wybierz liczbe stron ktora chcesz przeanalizowac: '))\n",
        "  return bike_categories[user_category_choice], user_page_range_choice"
      ],
      "metadata": {
        "id": "vz-YvAO8q1m7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_data():\n",
        "    bike_categories = find_bikes_categories()\n",
        "    user_bike_category_choice, user_page_range_choice = user_category_and_page_range_choice(bike_categories)\n",
        "    url_list = find_all_offerts_in_category(subcategory_html = user_bike_category_choice[1], page_range = user_page_range_choice)\n",
        "    #url_list = find_all_offerts_in_category(subcategory_html = bike_categories[5], page_range = 6 )\n",
        "    print(\"Zapytanie jest wyszukiwane, poczekajod 30 sekund do 3 minut\")\n",
        "    start_time = time.time()\n",
        "    products = search_offerts(url_list)\n",
        "    print(time.time() - start_time, \"seconds\")\n",
        "    output_name = f\"result {user_bike_category_choice[0]}.xlsx\"\n",
        "    return products, output_name\n",
        "\n",
        "def prepare_text_to_analysis(df):\n",
        "  text_to_analysis = []\n",
        "  for i in range(len(df)):\n",
        "    if df.opis[i] is not None and df.tytuł is not None:\n",
        "      text = df.tytuł[i] + df.opis[i]\n",
        "    elif df.tytuł is not None:\n",
        "      text = df.tytuł[i]\n",
        "    if text:\n",
        "      text_to_analysis.append(text.lower())\n",
        "    else:\n",
        "      text_to_analysis.append('')\n",
        "  return text_to_analysis\n",
        "\n",
        "def preprocess_data(products) -> pd.DataFrame:\n",
        "  df = pd.DataFrame(products, columns = ['link','tytuł', 'cena', 'opis'])  \n",
        "  text_to_analysis = prepare_text_to_analysis(df)\n",
        "\n",
        "  df['rozmiar koła'] = find_wheel_size(text_to_analysis)\n",
        "  df['rozmiar_ramy'] = find_frame_size(text_to_analysis)\n",
        "  df['producent'] = find_producent(text_to_analysis)\n",
        "  df['waga'] = find_bike_weight(text_to_analysis)\n",
        "  df['rok produkcji'] = find_bike_production_year(text_to_analysis)\n",
        "\n",
        "  opis_column = df.pop('opis')\n",
        "  df.insert(len(df.columns), 'opis', opis_column)\n",
        "  return df\n",
        "\n",
        "def filter_data(df):\n",
        "  \n",
        "  pass\n",
        "\n",
        "def main():\n",
        "  products, output_name = scrap_data()\n",
        "  df = preprocess_data(products)\n",
        "  df.drop_duplicates(subset=['link'], keep='first')\n",
        "  df = df.sort_values(by='cena')\n",
        "  df.to_excel(output_name)\n"
      ],
      "metadata": {
        "id": "xR8Cur0FFw9g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run main to use the program"
      ],
      "metadata": {
        "id": "r3veu-QftDd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "a1vJ8Qc7_YmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628eee23-0d14-49be-b0a5-76d04f043277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: rowery crossowe\n",
            "2: rowery dzieciece\n",
            "3: rowery elektryczne\n",
            "4: rowery miejskie\n",
            "5: rowery gorskie\n",
            "6: rowery szosowe\n",
            "7: rowery trekkingowe\n",
            "8: rowery gravelowe\n",
            "Wybierz kategorię od 1 do 8: 5\n",
            "Wybierz liczbe stron ktora chcesz przeanalizowac: 25\n",
            "------\n",
            "Zapytanie jest wyszukiwane, poczekajod 30 sekund do 3 minut\n",
            "260.7213325500488 seconds\n"
          ]
        }
      ]
    }
  ]
}